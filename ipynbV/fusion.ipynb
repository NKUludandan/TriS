{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import nltk\n",
    "import numpy as np\n",
    "import torch\n",
    "import json\n",
    "import math\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from sklearn.cluster import DBSCAN\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertModel, BertTokenizer\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import cmudict, stopwords\n",
    "import string\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class dataset(DataLoader):\n",
    "    def __init__(self, data_root, setlen):\n",
    "        self.text_path = []\n",
    "        self.label_path = []\n",
    "        for i in range(setlen):\n",
    "            if setlen==900 and (i == 70 or i==259):\n",
    "                continue\n",
    "            self.text_path.append(data_root+'problem-{}.txt'.format(str(i+1)))\n",
    "            self.label_path.append(data_root+'truth-problem-{}.json'.format(str(i+1)))\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.text_path)\n",
    "    \n",
    "    def __getitem__(self, item):\n",
    "        paragraphs = []\n",
    "        for line in open(self.text_path[item]):\n",
    "            paragraphs.append(line)\n",
    "\n",
    "        with open(self.label_path[item]) as json_file:\n",
    "            truth = json.load(json_file)\n",
    "\n",
    "        return (paragraphs, truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "dataset1: training 4200, validation 900\n",
    "dataset2: training 4200, validation 900\n",
    "dataset3: training 4200, validation 900\n",
    "label format: {'author': int -> number of authors occur in this file \n",
    "                'changes': int list -> length equals to num_paragraghs-1\n",
    "                                        every time a new paragragh appears-> 0=unchanged, 1=changed\n",
    "                }\n",
    "\"\"\"\n",
    "training_path1 = \"./release/pan23-multi-author-analysis-dataset1/pan23-multi-author-analysis-dataset1-train/\"\n",
    "val_path1 = \"./release/pan23-multi-author-analysis-dataset1/pan23-multi-author-analysis-dataset1-validation/\"\n",
    "training_path2 = \"./release/pan23-multi-author-analysis-dataset2/pan23-multi-author-analysis-dataset2-train/\"\n",
    "val_path2 = \"./release/pan23-multi-author-analysis-dataset2/pan23-multi-author-analysis-dataset2-validation/\"\n",
    "training_path3 = \"./release/pan23-multi-author-analysis-dataset3/pan23-multi-author-analysis-dataset3-train/\"\n",
    "val_path3 = \"./release/pan23-multi-author-analysis-dataset3/pan23-multi-author-analysis-dataset3-validation/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Training_set2 = dataset(data_root=training_path2, setlen=4200)\n",
    "trainingloader2 = DataLoader(dataset=Training_set2,batch_size=1,shuffle=True)\n",
    "Val_set2 = dataset(data_root=val_path2,setlen=900)\n",
    "valloader2 = DataLoader(dataset=Val_set2,batch_size=1,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StyleSpy(nn.Module):\n",
    "    def __init__(self, n_features=512, hidden_size=1024,padding='max_length', dropout=0.1):\n",
    "        super(StyleSpy,self).__init__()\n",
    "        self.padding = padding\n",
    "        self.berttokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "        self.pooling = nn.AdaptiveAvgPool1d(1)\n",
    "        self.ffn_hidden = nn.Sequential(nn.Linear(self.bert.config.hidden_size, hidden_size),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Dropout(dropout),\n",
    "                    nn.Linear(hidden_size, n_features),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Dropout(dropout),\n",
    "                    nn.Linear(n_features, n_features),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Dropout(dropout),\n",
    "                    nn.LayerNorm(normalized_shape=n_features)\n",
    "        )\n",
    "\n",
    "        self.ffn_cls = nn.Sequential(nn.Linear(self.bert.config.hidden_size, hidden_size),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Dropout(dropout),\n",
    "                    nn.Linear(hidden_size, n_features),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Dropout(dropout),\n",
    "                    nn.Linear(n_features, n_features),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Dropout(dropout),\n",
    "                    nn.LayerNorm(normalized_shape=n_features)\n",
    "        )\n",
    "\n",
    "        \n",
    "        # Freeze the BERT part\n",
    "        for param in self.bert.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        input_ids = self.berttokenizer.encode(text, add_special_tokens=True, padding=self.padding, truncation=True, max_length=256)\n",
    "        attention_mask = [int(id > 0) for id in input_ids]\n",
    "\n",
    "        input_ids = torch.tensor(input_ids).unsqueeze(0).to(device)\n",
    "        attention_mask = torch.tensor(attention_mask).unsqueeze(0).to(device)\n",
    "\n",
    "        return input_ids, attention_mask\n",
    "\n",
    "    def forward(self, text):\n",
    "        input_ids, attention_mask = self.tokenize(text)\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        hidden_state = outputs.last_hidden_state[:,1:-1,:]\n",
    "        hidden_state = self.pooling(hidden_state.permute(0, 2, 1)).permute(0, 2, 1).squeeze(1)\n",
    "        cls_token = outputs.pooler_output\n",
    "\n",
    "        hidden_state = self.ffn_hidden(hidden_state)\n",
    "        #cls_token = self.ffn_cls(cls_token)\n",
    "        #features = torch.cat((cls_token,hidden_state),dim=1)\n",
    "\n",
    "        return hidden_state #features\n",
    "\n",
    "\n",
    "class BERT_MLP_Model(nn.Module):\n",
    "    def __init__(self, num_classes=512):\n",
    "        super(BERT_MLP_Model, self).__init__()\n",
    "        self.nltktokenizer = nltk.word_tokenize\n",
    "        self.pos_tagger = nltk.pos_tag\n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "        self.berttokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "        self.pooling = nn.AdaptiveAvgPool1d(1)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(768, 1024),  # BERT的输出维度为768\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, num_classes),\n",
    "            nn.LayerNorm(num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, text):\n",
    "        tokens = self.nltktokenizer(text)\n",
    "        pos_tags = self.pos_tagger(tokens)\n",
    "        pos_tags = [p for w, p in pos_tags]\n",
    "        #inputs = ['[CLS]'] + pos_tags + ['[SEP]']\n",
    "        input_ids = self.berttokenizer.encode(pos_tags, max_length=256,truncation=True)\n",
    "        input_ids_tensor = torch.tensor([input_ids]).to(device)\n",
    "        #print(input_ids_tensor.shape)\n",
    "        outputs = self.bert(input_ids_tensor).last_hidden_state[:,1:-1,:]\n",
    "        outputs = self.pooling(outputs.permute(0, 2, 1)).permute(0, 2, 1).squeeze(1)\n",
    "        outputs = self.mlp(outputs)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Get_Sty_Features(text):\n",
    "    # avg word len\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    total_chars = sum(len(word) for word in tokens)\n",
    "    average_word_length = total_chars / len(tokens)\n",
    "    # avg sentence len\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    total_words = sum(len(nltk.word_tokenize(sentence)) for sentence in sentences)\n",
    "    average_sentence_length = total_words / len(sentences)\n",
    "    # para len by chars\n",
    "    paragraph_length_chars = len(text)\n",
    "    # para len by tokens\n",
    "    paragraph_length_words = len(tokens)\n",
    "    # para len by sentences\n",
    "    paragraph_length_sents = len(sentences)\n",
    "    # Type-Token Ratio\n",
    "    total_tokens = len(tokens)\n",
    "    unique_types = len(set(tokens))\n",
    "    type_token_ratio = unique_types / total_tokens\n",
    "    # avg syllables\n",
    "    cmud = cmudict.dict()\n",
    "    total_syllables = sum([len(cmud.get(word.lower(), [[None]])[0]) for word in tokens])\n",
    "    average_syllables_per_word = total_syllables / len(tokens)\n",
    "    # Flesch-Kincaid readability score\n",
    "    flesch_reading_ease = 206.835 - (1.015 * average_sentence_length) - (84.6 * average_syllables_per_word)\n",
    "    # Stopwords Count\n",
    "    stopwords_list = stopwords.words(\"english\")\n",
    "    stopwords_count = len([token for token in tokens if token.lower() in stopwords_list])\n",
    "    # Function words Count\n",
    "    function_words = nltk.pos_tag(tokens)\n",
    "    function_words_count = len([word for word, pos in function_words if pos.startswith(\"FW\")])\n",
    "    # Punctuation Marks Ratio\n",
    "    punctuation_count = sum([1 for token in tokens if token in string.punctuation])\n",
    "    punctuation_ratio = punctuation_count / len(tokens)\n",
    "    return [average_word_length, average_sentence_length, paragraph_length_chars, \n",
    "            paragraph_length_words, paragraph_length_sents, type_token_ratio, \n",
    "            average_syllables_per_word, flesch_reading_ease, stopwords_count, \n",
    "            function_words_count, punctuation_ratio]\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Sem = torch.load(\"./checkpoints/best_0.7297875374304862_acc_0.6318243637070139_F1_4_thre.pth\").to(device)\n",
    "Syn = torch.load(\"./checkpoints/best_tag_0.6188507058320263_acc_0.4705882352941176_F1.pth\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.load('./trainingFea.pth')\n",
    "Sty = LogisticRegression(max_iter=100000)\n",
    "X = a['features']\n",
    "Y = a['labels']\n",
    "X = [t.tolist() for t in X]\n",
    "Y = [t.item() for t in Y]\n",
    "Sty.fit(X,Y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre = []\n",
    "tar = []\n",
    "cos_sim = nn.CosineSimilarity(dim=1, eps=1e-6)\n",
    "loop = tqdm(enumerate(valloader2, start=len(valloader2)), total=len(valloader2), leave=False)\n",
    "for step, (paragraphs, truth) in loop:\n",
    "    para_result = []\n",
    "    sem_embeddings = []\n",
    "    syn_embeddings = []\n",
    "    for para in paragraphs:\n",
    "        sem_embeddings.append(Sem(para[0]))\n",
    "        syn_embedding = Syn(para[0])\n",
    "        syn_embedding = F.normalize(syn_embedding, p=2, dim=1)\n",
    "        syn_embeddings.append(syn_embedding)\n",
    "\n",
    "    weights = truth['changes']\n",
    "    for i, weight in enumerate(weights):\n",
    "        tar.append(weight.item())\n",
    "\n",
    "        sem_score = torch.cdist(sem_embeddings[i], sem_embeddings[i+1]).item()\n",
    "        syn_score = cos_sim(syn_embeddings[i], syn_embeddings[i+1]).item()\n",
    "\n",
    "        sty1 = Get_Sty_Features(paragraphs[i][0])\n",
    "        sty2 = Get_Sty_Features(paragraphs[i+1][0])\n",
    "        sty = [abs(x-y) for x, y in zip(sty1,sty2)]\n",
    "        sty_score = Sty.predict([sty])\n",
    "\n",
    "        p = 0 #less than 0: unchange\n",
    "        p = p + 0.9*(sem_score-3.1) - 0.3*(syn_score-0.9) + 0.35*(sty_score-0.5)\n",
    "        pre.append(int(p>0))\n",
    "    acc = accuracy_score(tar, pre)\n",
    "    f1 = f1_score(tar, pre, average='binary')\n",
    "    print(f\"F1 score: {f1:.4f}, acc_para: {acc:.4f}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ADL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
