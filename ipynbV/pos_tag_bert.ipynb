{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import nltk\n",
    "import numpy as np\n",
    "import torch\n",
    "import json\n",
    "import math\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from sklearn.cluster import DBSCAN\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertModel, BertTokenizer\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class dataset(DataLoader):\n",
    "    def __init__(self, data_root, setlen):\n",
    "        self.text_path = []\n",
    "        self.label_path = []\n",
    "        for i in range(setlen):\n",
    "            if setlen==900 and (i == 70 or i==259):\n",
    "                continue\n",
    "            self.text_path.append(data_root+'problem-{}.txt'.format(str(i+1)))\n",
    "            self.label_path.append(data_root+'truth-problem-{}.json'.format(str(i+1)))\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.text_path)\n",
    "    \n",
    "    def __getitem__(self, item):\n",
    "        paragraphs = []\n",
    "        for line in open(self.text_path[item]):\n",
    "            paragraphs.append(line)\n",
    "\n",
    "        with open(self.label_path[item]) as json_file:\n",
    "            truth = json.load(json_file)\n",
    "\n",
    "        return (paragraphs, truth)\n",
    "                \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERT_MLP_Model(nn.Module):\n",
    "    def __init__(self, num_classes=512):\n",
    "        super(BERT_MLP_Model, self).__init__()\n",
    "        self.nltktokenizer = nltk.word_tokenize\n",
    "        self.pos_tagger = nltk.pos_tag\n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "        self.berttokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "        self.pooling = nn.AdaptiveAvgPool1d(1)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(768, 1024),  # BERT的输出维度为768\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, num_classes),\n",
    "            nn.LayerNorm(num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, text):\n",
    "        tokens = self.nltktokenizer(text)\n",
    "        pos_tags = self.pos_tagger(tokens)\n",
    "        pos_tags = [p for w, p in pos_tags]\n",
    "        #inputs = ['[CLS]'] + pos_tags + ['[SEP]']\n",
    "        input_ids = self.berttokenizer.encode(pos_tags, max_length=256,truncation=True)\n",
    "        input_ids_tensor = torch.tensor([input_ids]).to(device)\n",
    "        #print(input_ids_tensor.shape)\n",
    "        outputs = self.bert(input_ids_tensor).last_hidden_state[:,1:-1,:]\n",
    "        outputs = self.pooling(outputs.permute(0, 2, 1)).permute(0, 2, 1).squeeze(1)\n",
    "        outputs = self.mlp(outputs)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 512])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = BERT_MLP_Model().to(device)\n",
    "model('I like apple.').shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "dataset1: training 4200, validation 900\n",
    "dataset2: training 4200, validation 900\n",
    "dataset3: training 4200, validation 900\n",
    "label format: {'author': int -> number of authors occur in this file \n",
    "                'changes': int list -> length equals to num_paragraghs-1\n",
    "                                        every time a new paragragh appears-> 0=unchanged, 1=changed\n",
    "                }\n",
    "\"\"\"\n",
    "training_path1 = \"./release/pan23-multi-author-analysis-dataset1/pan23-multi-author-analysis-dataset1-train/\"\n",
    "val_path1 = \"./release/pan23-multi-author-analysis-dataset1/pan23-multi-author-analysis-dataset1-validation/\"\n",
    "training_path2 = \"./release/pan23-multi-author-analysis-dataset2/pan23-multi-author-analysis-dataset2-train/\"\n",
    "val_path2 = \"./release/pan23-multi-author-analysis-dataset2/pan23-multi-author-analysis-dataset2-validation/\"\n",
    "training_path3 = \"./release/pan23-multi-author-analysis-dataset3/pan23-multi-author-analysis-dataset3-train/\"\n",
    "val_path3 = \"./release/pan23-multi-author-analysis-dataset3/pan23-multi-author-analysis-dataset3-validation/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "Training_set2 = dataset(data_root=training_path2, setlen=4200)\n",
    "trainingloader2 = DataLoader(dataset=Training_set2,batch_size=1,shuffle=True)\n",
    "Val_set2 = dataset(data_root=val_path2,setlen=900)\n",
    "valloader2 = DataLoader(dataset=Val_set2,batch_size=1,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(truth, para_embeddings, threshold=4):\n",
    "    #sim_loss = nn.MSELoss()\n",
    "    weights = truth['changes']#.to(device)\n",
    "    n = truth['authors'].to(device)\n",
    "    d_loss = 0.0\n",
    "    for i, weight in enumerate(weights):\n",
    "        weight = weight.to(device)\n",
    "        D2 = torch.cdist(para_embeddings[i], para_embeddings[i+1])\n",
    "        #print(D2)\n",
    "        l = (1-weight) * D2 + weight * torch.max(threshold - D2, torch.tensor(0.0).to(device))\n",
    "        # if weight == 0:\n",
    "        #     d_loss = sim_loss(para_embeddings[i], para_embeddings[i+1])\n",
    "        # if weight == 1:\n",
    "        #     d = 1 - sim_loss(para_embeddings[i], para_embeddings[i+1])\n",
    "        #     d_loss = torch.max(d, torch.tensor(0.0).to(device))\n",
    "        # d_loss = d_loss if d_loss<threshold else 2*d_loss\n",
    "        d_loss = d_loss + l\n",
    "    d_loss = d_loss/len(weights)\n",
    "\n",
    "    # embs = torch.cat(para_embeddings,dim=0)\n",
    "    # distances = torch.cdist(embs,embs,p=2,compute_mode='donot_use_mm_for_euclid_dist')\n",
    "    # clusters = torch.zeros(embs.shape[0], dtype=torch.long)\n",
    "    # cluster_count = torch.Tensor([0]).to(device)\n",
    "    # cluster_count.requires_grad=True\n",
    "    # for i in range(embs.shape[0]):\n",
    "    #     if clusters[i] != 0:\n",
    "    #         continue\n",
    "    #     cluster_count = cluster_count + 1\n",
    "    #     clusters[i] = cluster_count\n",
    "\n",
    "    #     for j in range(embs.shape[0]):\n",
    "    #         if distances[i, j] <= 0.5:\n",
    "    #             if clusters[j] == 0:\n",
    "    #                 clusters[j] = cluster_count\n",
    "\n",
    "    #distances = F.pairwise_distance(torch.cat(tensors, dim=0), torch.cat(tensors, dim=0))\n",
    "\n",
    "    # c_loss = abs(cluster_count-n)\n",
    "    #print(c_loss)\n",
    "    \n",
    "    return 10*d_loss #+ c_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, loader, threshold=4):\n",
    "    cos_sim = nn.CosineSimilarity(dim=1, eps=1e-6)\n",
    "    acc_txt_count = 0\n",
    "    txt_count = 0\n",
    "    acc_class_count = 0\n",
    "    para_results = []\n",
    "    true_label = []\n",
    "\n",
    "    loop = tqdm(enumerate(loader, start=len(loader)), total=len(loader), leave=False)\n",
    "    for step, (paragraphs, truth) in loop:\n",
    "        para_result = []\n",
    "        para_embeddings = []\n",
    "        for para in paragraphs:\n",
    "            #tags = tagger(para[0])\n",
    "            para_embedding = model(para[0])\n",
    "            para_embedding = F.normalize(para_embedding, p=2, dim=1)\n",
    "            para_embeddings.append(para_embedding)\n",
    "        \n",
    "        n = truth['authors'].to(device)\n",
    "        weights = truth['changes']\n",
    "        for i, weight in enumerate(weights):\n",
    "            score = cos_sim(para_embeddings[i], para_embeddings[i+1])\n",
    "            #print(score)\n",
    "            para_results.append(int(score<0))\n",
    "            true_label.append(weight.item())\n",
    "        \n",
    "\n",
    "        # acc_txt_count = acc_txt_count + all(x==y for x,y in zip(weights,para_result))\n",
    "        # txt_count = txt_count + 1\n",
    "\n",
    "        # dbscan = DBSCAN(eps=0.5, min_samples=1,metric='precomputed')\n",
    "        # embs = torch.cat(para_embeddings,dim=0)\n",
    "        # distances = torch.cdist(embs,embs,p=2,compute_mode='donot_use_mm_for_euclid_dist')\n",
    "        # clusters = torch.zeros(embs.shape[0], dtype=torch.long)\n",
    "        # cluster_count = torch.Tensor([0]).to(device)\n",
    "        # cluster_count.requires_grad=True\n",
    "        # for i in range(embs.shape[0]):\n",
    "        #     if clusters[i] != 0:\n",
    "        #         continue\n",
    "        #     cluster_count = cluster_count + 1\n",
    "        #     clusters[i] = cluster_count\n",
    "\n",
    "        #     for j in range(embs.shape[0]):\n",
    "        #         if distances[i, j] <= 0.5:\n",
    "        #             if clusters[j] == 0:\n",
    "        #                 clusters[j] = cluster_count\n",
    "        # if cluster_count == n: acc_class_count+=1\n",
    "\n",
    "        # para_results = para_results+para_result\n",
    "\n",
    "    F1 = f1_score(para_results, true_label)\n",
    "    acc_para = sum(x==y for x,y in zip(para_results, true_label))\n",
    "    acc_para = acc_para/len(para_results)\n",
    "    # acc_txt = acc_txt_count/txt_count\n",
    "    # acc_cluster = acc_class_count/txt_count\n",
    "\n",
    "    print(f\"F1 score: {F1:.4f}, acc_para: {acc_para:.4f}\")#, acc_txt: {acc_txt:.4f}, acc_clu: {acc_cluster:.4f} \")\n",
    "    \n",
    "    return F1,acc_para# ,acc_txt,acc_class_count\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, trainloader, valloader, epochs, optimizer, threshold, save_freq, loss_fn=loss_fn):\n",
    "    cos_sim = nn.CosineSimilarity(dim=1, eps=1e-6)\n",
    "    writer = SummaryWriter(\"./log/\")\n",
    "    low = torch.tensor(0.0).to(device)\n",
    "    acc = 0.0\n",
    "    for epoch in range(epochs):\n",
    "        loop = tqdm(enumerate(trainloader, start=epoch * len(trainloader)), total=len(trainloader), leave=False)\n",
    "        for step, (paragraphs, truth) in loop:\n",
    "            optimizer.param_groups[0]['lr'] = 0.0000001#adjust_learning_rate(epochs, batch_size, trainloader, step)\n",
    "            optimizer.zero_grad()\n",
    "            weights = truth['changes']\n",
    "            for i, weight in enumerate(weights):\n",
    "                weight = weight.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                f1 = model(paragraphs[i][0])\n",
    "                f1 = F.normalize(f1, p=2, dim=1)\n",
    "                f2 = model(paragraphs[i+1][0])\n",
    "                f2 = F.normalize(f2, p=2, dim=1)\n",
    "                cs = cos_sim(f1,f2)\n",
    "                loss = (1-weight) * (1-cs) + weight * (cs+1)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            writer.add_scalar(\"Loss/train\", loss, epoch)\n",
    "\n",
    "            if step % int(save_freq) == 0 and step:\n",
    "                with open(os.path.join(\"./log/\", 'logs.txt'), 'a') as log_file:\n",
    "                    log_file.write(f'Epoch: {epoch}, Step: {step}, Train loss: {loss.cpu().detach().numpy()} \\n')\n",
    "\n",
    "                state = dict(epoch=epoch + 1, model=model.state_dict(),\n",
    "                         optimizer=optimizer.state_dict())\n",
    "\n",
    "                #torch.save(state, os.path.join('.', 'checkpoints', f'checkpoint_{step}_steps.pth'))\n",
    "            if step % 4000 == 0 and step:\n",
    "                with torch.no_grad():\n",
    "                    F1,acc_para= evaluate(model,valloader,threshold=threshold)\n",
    "                    if acc_para > acc:\n",
    "                        acc = acc_para\n",
    "                        torch.save(model, os.path.join('.', 'checkpoints', f'b est_tag_{acc}_acc_{F1}_F1.pth'))\n",
    "            loop.set_description(f'Epoch [{epoch}/{epochs}]')\n",
    "            loop.set_postfix(loss = loss.cpu().detach().numpy())\n",
    "            \n",
    "        print(f'Loss for epoch {epoch} is {loss.cpu().detach().numpy()}')\n",
    "    print('End of the Training. Saving final checkpoints.')\n",
    "    state = dict(epoch=epochs, model=model.state_dict(),\n",
    "                 optimizer=optimizer.state_dict())\n",
    "    torch.save(state, os.path.join('.', 'checkpoints',  'final_checkpoint.pth'))\n",
    "    writer.flush()\n",
    "    writer.close()    \n",
    "                \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = BERT_MLP_Model().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()),\n",
    "                lr=0.1,\n",
    "                betas=(0.9, 0.999),\n",
    "                eps=1e-08,\n",
    "                weight_decay=0,\n",
    "                amsgrad=False)\n",
    "epochs = 100\n",
    "lr = 0.1\n",
    "batch_size = 1\n",
    "threshold = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(model,trainingloader2,valloader2,epochs,optimizer,threshold,1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ADL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
