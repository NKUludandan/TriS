{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import json\n",
    "import math\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from sklearn.cluster import DBSCAN\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertModel, BertTokenizer\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class dataset(DataLoader):\n",
    "    def __init__(self, data_root, setlen):\n",
    "        self.text_path = []\n",
    "        self.label_path = []\n",
    "        for i in range(setlen):\n",
    "            if setlen==900 and (i == 70 or i==259):\n",
    "                continue\n",
    "            self.text_path.append(data_root+'problem-{}.txt'.format(str(i+1)))\n",
    "            self.label_path.append(data_root+'truth-problem-{}.json'.format(str(i+1)))\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.text_path)\n",
    "    \n",
    "    def __getitem__(self, item):\n",
    "        paragraphs = []\n",
    "        for line in open(self.text_path[item]):\n",
    "            paragraphs.append(line)\n",
    "\n",
    "        with open(self.label_path[item]) as json_file:\n",
    "            truth = json.load(json_file)\n",
    "\n",
    "        return (paragraphs, truth)\n",
    "                \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StyleSpy(nn.Module):\n",
    "    def __init__(self, n_features=512, hidden_size=1024,padding='max_length', dropout=0.1):\n",
    "        super(StyleSpy,self).__init__()\n",
    "        self.padding = padding\n",
    "        self.berttokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "        self.pooling = nn.AdaptiveAvgPool1d(1)\n",
    "        self.ffn_hidden = nn.Sequential(nn.Linear(self.bert.config.hidden_size, hidden_size),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Dropout(dropout),\n",
    "                    nn.Linear(hidden_size, n_features),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Dropout(dropout),\n",
    "                    nn.Linear(n_features, n_features),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Dropout(dropout),\n",
    "                    nn.LayerNorm(normalized_shape=n_features)\n",
    "        )\n",
    "\n",
    "        self.ffn_cls = nn.Sequential(nn.Linear(self.bert.config.hidden_size, hidden_size),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Dropout(dropout),\n",
    "                    nn.Linear(hidden_size, n_features),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Dropout(dropout),\n",
    "                    nn.Linear(n_features, n_features),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Dropout(dropout),\n",
    "                    nn.LayerNorm(normalized_shape=n_features)\n",
    "        )\n",
    "\n",
    "        \n",
    "        # Freeze the BERT part\n",
    "        for param in self.bert.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        input_ids = self.berttokenizer.encode(text, add_special_tokens=True, padding=self.padding, truncation=True, max_length=256)\n",
    "        attention_mask = [int(id > 0) for id in input_ids]\n",
    "\n",
    "        input_ids = torch.tensor(input_ids).unsqueeze(0).to(device)\n",
    "        attention_mask = torch.tensor(attention_mask).unsqueeze(0).to(device)\n",
    "\n",
    "        return input_ids, attention_mask\n",
    "\n",
    "    def forward(self, text):\n",
    "        input_ids, attention_mask = self.tokenize(text)\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        hidden_state = outputs.last_hidden_state[:,1:-1,:]\n",
    "        hidden_state = self.pooling(hidden_state.permute(0, 2, 1)).permute(0, 2, 1).squeeze(1)\n",
    "        cls_token = outputs.pooler_output\n",
    "\n",
    "        hidden_state = self.ffn_hidden(hidden_state)\n",
    "        cls_token = self.ffn_cls(cls_token)\n",
    "        features = torch.cat((cls_token,hidden_state),dim=1)\n",
    "\n",
    "        return features\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_learning_rate(epochs, batch_size, loader, step):\n",
    "    max_steps = epochs * len(loader)\n",
    "    warmup_steps = 10 * len(loader)\n",
    "    base_lr = batch_size / 256\n",
    "    if step < warmup_steps:\n",
    "        lr = base_lr * step / warmup_steps\n",
    "    else:\n",
    "        step -= warmup_steps\n",
    "        max_steps -= warmup_steps\n",
    "        q = 0.5 * (1 + math.cos(math.pi * step / max_steps))\n",
    "        end_lr = base_lr * 0.001\n",
    "        lr = base_lr * q + end_lr * (1 - q)\n",
    "    return lr * lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(truth, para_embeddings, threshold=4):\n",
    "    #sim_loss = nn.MSELoss()\n",
    "    d_loss = 0.0\n",
    "    weights = truth['changes']\n",
    "    n = truth['authors'].to(device)\n",
    "    for i, weight in enumerate(weights):\n",
    "        weight = weight.to(device)\n",
    "        D2 = torch.cdist(para_embeddings[i], para_embeddings[i+1])\n",
    "        l = (1-weight) * D2 + weight * torch.max(threshold - D2, torch.tensor(0.0).to(device))\n",
    "        d_loss = d_loss + l\n",
    "    d_loss = d_loss/len(weights)\n",
    "\n",
    "    # embs = torch.cat(para_embeddings,dim=0)\n",
    "    # distances = torch.cdist(embs,embs,p=2,compute_mode='donot_use_mm_for_euclid_dist')\n",
    "    # clusters = torch.zeros(embs.shape[0], dtype=torch.long)\n",
    "    # cluster_count = torch.Tensor([0]).to(device)\n",
    "    # cluster_count.requires_grad=True\n",
    "    # for i in range(embs.shape[0]):\n",
    "    #     if clusters[i] != 0:\n",
    "    #         continue\n",
    "    #     cluster_count = cluster_count + 1\n",
    "    #     clusters[i] = cluster_count\n",
    "\n",
    "    #     for j in range(embs.shape[0]):\n",
    "    #         if distances[i, j] <= 0.5:\n",
    "    #             if clusters[j] == 0:\n",
    "    #                 clusters[j] = cluster_count\n",
    "\n",
    "    #distances = F.pairwise_distance(torch.cat(tensors, dim=0), torch.cat(tensors, dim=0))\n",
    "\n",
    "    # c_loss = abs(cluster_count-n)\n",
    "    #print(c_loss)\n",
    "    \n",
    "    return d_loss #+ c_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, loader, threshold=4):\n",
    "    \n",
    "    acc_txt_count = 0\n",
    "    txt_count = 0\n",
    "    acc_class_count = 0\n",
    "    para_results = []\n",
    "    true_label = []\n",
    "\n",
    "    loop = tqdm(enumerate(loader, start=len(loader)), total=len(loader), leave=False)\n",
    "    for step, (paragraphs, truth) in loop:\n",
    "        para_result = []\n",
    "        para_embeddings = []\n",
    "        for para in paragraphs:\n",
    "            para_embedding = model(para[0])\n",
    "            para_embeddings.append(para_embedding)\n",
    "        \n",
    "        n = truth['authors'].to(device)\n",
    "        weights = truth['changes']\n",
    "        for i, weight in enumerate(weights):\n",
    "            score = torch.cdist(para_embeddings[i], para_embeddings[i+1])\n",
    "            #print(score)\n",
    "            para_result.append(int(score>threshold))\n",
    "            true_label.append(weight.item())\n",
    "        \n",
    "\n",
    "        # acc_txt_count = acc_txt_count + all(x==y for x,y in zip(weights,para_result))\n",
    "        # txt_count = txt_count + 1\n",
    "\n",
    "        # dbscan = DBSCAN(eps=0.5, min_samples=1,metric='precomputed')\n",
    "        # embs = torch.cat(para_embeddings,dim=0)\n",
    "        # distances = torch.cdist(embs,embs,p=2,compute_mode='donot_use_mm_for_euclid_dist')\n",
    "        # clusters = torch.zeros(embs.shape[0], dtype=torch.long)\n",
    "        # cluster_count = torch.Tensor([0]).to(device)\n",
    "        # cluster_count.requires_grad=True\n",
    "        # for i in range(embs.shape[0]):\n",
    "        #     if clusters[i] != 0:\n",
    "        #         continue\n",
    "        #     cluster_count = cluster_count + 1\n",
    "        #     clusters[i] = cluster_count\n",
    "\n",
    "        #     for j in range(embs.shape[0]):\n",
    "        #         if distances[i, j] <= 0.5:\n",
    "        #             if clusters[j] == 0:\n",
    "        #                 clusters[j] = cluster_count\n",
    "        # if cluster_count == n: acc_class_count+=1\n",
    "\n",
    "        para_results = para_results+para_result\n",
    "\n",
    "    F1 = f1_score(para_results, true_label)\n",
    "    acc_para = sum(x==y for x,y in zip(para_results, true_label))\n",
    "    acc_para = acc_para/len(para_results)\n",
    "    # acc_txt = acc_txt_count/txt_count\n",
    "    # acc_cluster = acc_class_count/txt_count\n",
    "\n",
    "    #print(f\"F1 score: {F1:.4f}, acc_para: {acc_para:.4f}, acc_txt: {acc_txt:.4f}, acc_clu: {acc_cluster:.4f} \")\n",
    "    print(f\"F1 score: {F1:.4f}, acc_para: {acc_para:.4f}\")\n",
    "    return F1,acc_para#,acc_txt,acc_class_count\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, trainloader, valloader, epochs, optimizer, threshold, save_freq, loss_fn=loss_fn):\n",
    "    sim_loss = nn.MSELoss()\n",
    "    writer = SummaryWriter(\"./log/\")\n",
    "    acc = 0.0\n",
    "    for epoch in range(epochs):\n",
    "        loop = tqdm(enumerate(trainloader, start=epoch * len(trainloader)), total=len(trainloader), leave=False)\n",
    "        for step, (paragraphs, truth) in loop:\n",
    "            optimizer.param_groups[0]['lr'] = 0.000001#adjust_learning_rate(epochs, batch_size, trainloader, step)\n",
    "            optimizer.zero_grad()\n",
    "            para_embeddings = []\n",
    "            for para in paragraphs:\n",
    "                para_embedding = model(para[0])\n",
    "                para_embeddings.append(para_embedding)\n",
    "            \n",
    "            loss = loss_fn(truth, para_embeddings,threshold=threshold)\n",
    "            \n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            writer.add_scalar(\"Loss/train\", loss, epoch)\n",
    "\n",
    "            if step % int(save_freq) == 0 and step:\n",
    "                with open(os.path.join(\"./log/\", 'logs.txt'), 'a') as log_file:\n",
    "                    log_file.write(f'Epoch: {epoch}, Step: {step}, Train loss: {loss.cpu().detach().numpy()} \\n')\n",
    "\n",
    "                state = dict(epoch=epoch + 1, model=model.state_dict(),\n",
    "                         optimizer=optimizer.state_dict())\n",
    "\n",
    "                #torch.save(state, os.path.join('.', 'checkpoints', f'checkpoint_{step}_steps.pth'))\n",
    "            if step % 4000 == 0 and step:\n",
    "                with torch.no_grad():\n",
    "                    F1,acc_para = evaluate(model,valloader,threshold=threshold)\n",
    "                    if acc_para > acc:\n",
    "                        acc = acc_para\n",
    "                        torch.save(model, os.path.join('.', 'checkpoints', f'best_{acc}_acc_{F1}_F1_{threshold}_thre.pth'))\n",
    "            loop.set_description(f'Epoch [{epoch}/{epochs}]')\n",
    "            loop.set_postfix(loss = loss.cpu().detach().numpy())\n",
    "            \n",
    "        print(f'Loss for epoch {epoch} is {loss.cpu().detach().numpy()}')\n",
    "    print('End of the Training. Saving final checkpoints.')\n",
    "    state = dict(epoch=epochs, model=model.state_dict(),\n",
    "                 optimizer=optimizer.state_dict())\n",
    "    torch.save(state, os.path.join('.', 'checkpoints',  'final_checkpoint.pth'))\n",
    "    writer.flush()\n",
    "    writer.close()    \n",
    "                \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "dataset1: training 4200, validation 900\n",
    "dataset2: training 4200, validation 900\n",
    "dataset3: training 4200, validation 900\n",
    "label format: {'author': int -> number of authors occur in this file \n",
    "                'changes': int list -> length equals to num_paragraghs-1\n",
    "                                        every time a new paragragh appears-> 0=unchanged, 1=changed\n",
    "                }\n",
    "\"\"\"\n",
    "training_path1 = \"./release/pan23-multi-author-analysis-dataset1/pan23-multi-author-analysis-dataset1-train/\"\n",
    "val_path1 = \"./release/pan23-multi-author-analysis-dataset1/pan23-multi-author-analysis-dataset1-validation/\"\n",
    "training_path2 = \"./release/pan23-multi-author-analysis-dataset2/pan23-multi-author-analysis-dataset2-train/\"\n",
    "val_path2 = \"./release/pan23-multi-author-analysis-dataset2/pan23-multi-author-analysis-dataset2-validation/\"\n",
    "training_path3 = \"./release/pan23-multi-author-analysis-dataset3/pan23-multi-author-analysis-dataset3-train/\"\n",
    "val_path3 = \"./release/pan23-multi-author-analysis-dataset3/pan23-multi-author-analysis-dataset3-validation/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "lr = 0.1\n",
    "batch_size = 1\n",
    "threshold = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = torch.load('./checkpoints/best_0.7297875374304862_acc_0.6318243637070139_F1_4_thre.pth')\n",
    "# model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()),\n",
    "                lr=0.1,\n",
    "                betas=(0.9, 0.999),\n",
    "                eps=1e-08,\n",
    "                weight_decay=0,\n",
    "                amsgrad=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "Training_set1 = dataset(data_root=training_path1, setlen=4200)\n",
    "trainingloader1 = DataLoader(dataset=Training_set1,batch_size=1,shuffle=True)\n",
    "Val_set1 = dataset(data_root=val_path1,setlen=900)\n",
    "valloader1 = DataLoader(dataset=Val_set1,batch_size=1,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "Training_set2 = dataset(data_root=training_path2, setlen=4200)\n",
    "trainingloader2 = DataLoader(dataset=Training_set2,batch_size=1,shuffle=True)\n",
    "Val_set2 = dataset(data_root=val_path2,setlen=900)\n",
    "valloader2 = DataLoader(dataset=Val_set2,batch_size=1,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "Training_set3 = dataset(data_root=training_path3, setlen=4200)\n",
    "trainingloader3 = DataLoader(dataset=Training_set3,batch_size=1,shuffle=True)\n",
    "Val_set3 = dataset(data_root=val_path3,setlen=900)\n",
    "valloader3 = DataLoader(dataset=Val_set3,batch_size=1,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = StyleSpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(model,trainingloader2,valloader2,epochs,optimizer,threshold,1000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ADL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
